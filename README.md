# Dataset for machine-generated text detection in Russian

The dataset consists of a zip-archive with:
+  450k documents from Russian Wikipedia corpus as a real part for detection task
+  450k machine-generated documents. Generated documents contains 150k samples for each language model [SberAI-small](https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2), [SberAI-large](https://huggingface.co/sberbank-ai/rugpt3large_based_on_gpt2), [Facebook-XGLM](https://huggingface.co/facebook/xglm-1.7B) with 3 sampling methods (*top-k, top-p, pure*) and 2 priming sequence strategy (*word, sentence*).

## Download
We provide you with access both to the data with the combined generated samples and to the individual parts of the documents generated by the different language models.

By following path  `combined_data.zip` you will find:
+ real.json (450k real data)
+ generated.json (450k generated data)
  
By following path  `individual_parts.zip` you will find:
* real.json (450k real data)
* Folder for each language model with (150k generated data)
  * pure-sent.json
  + pure-word.json
  + top-p-sent.json
  + top-p-word.json
  + top-k-sent.json
  + top-k-word.json








___
## References
Cite this dataset: 
+ `Gritsay, German (2022), “Open access dataset for machine-generated text detection in Russian”, Mendeley Data, V1, doi: 10.17632/4ynxfp3w53.1`

